Explain.txt

Special features
1. Use multi-thread to download and parse the pages, in order to accelerate the speed. The speed is around 10-30 pages per second with 20 threads and 10MB internet connection.
2. Use configuration file to config the crawler. 
3. The MIME type filter, MaxThread, MaxCrawlerSeed and log file name are configurable.
4. Use MIME type to filter the pages.
5. Cache robots to improve the performance.
6. Use regrex to match the links so that it can get all the matched links even if the parsing html file is malformed.
7. Compress the pages and save as zip files.
8. Customize HttpHandler to add "gzip, deflate" as the "Accpet-Encoding", to reduce the download time if server supports.
9. Customize HttpHandler to log all the http errors and it will not be stuck or crashed.
10. Print the status as the progress bar.
11. Use singleton pattern for some classes to improve memroy efficiency.
12. Convert the relative url to absolute url and try to fix some malformed urls.
13. Set timeout for download.
14. Try best to deal with exceptions to make it stable.
15. Well code-architecture so that it can be refactored and extended easily.

Shortages
1. Only parse <a href> style links.
2. Doesn't deal with character encoding when saving the pages. So when open the downloaded pages some characters may be messed.
3. Doesn't save the current status when the program is forced to quit and it can't resume when restart.
4. May be banned by some sites when using multi-thread.
5. There is character encoding issue when parsing the urls.


To be improved
1. Improve the parser to deal with more cases.
2. Checkthe encoding of the pages to encode it accordingly when saving the pages.
3. Save the current status when forced quit and resume unfinished tasks.
4. Maintain a httpconnection pool to ease the side-effect of multi-thread.
5. Check the url encoding and use correct code when parsing the urls.
6. Refactor to make it can run on distributed machines.